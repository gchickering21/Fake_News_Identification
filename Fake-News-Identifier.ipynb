{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sentiment Analysis and Recurrent Neural Networks to Idenfify Fake News"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " One of the biggest issues facing society today is the increasing prevalence of fake news that can be found all across social media and across the internet. It is getting harder and harder to discern between what is real and what is fake, and it leaves people either questioning the truth and scientific knowledge or outright believing things that are not true. With people getting more and more of their news from social media such as Facebook and Twitter, the high amount of fake news on these platforms is deeply concerning and finding ways to decrease the spread of fake information is one of the most pressing issues that society and people working within the technology sector are facing. It has been said that false claims are 70% more likely to be shared on Twitter than real news, so finding ways to identify fake news early on before it can be spread has important consequences for what the future of society and democracy looks like.\n",
    "\n",
    "For my project it is exactly this issue of identifying fake news that my project focuses on. With the large amounts of fake news article that are already pervasive on the internet, the recent developments in machine learning appear to be one of the best ways to identify fake news sources and prevent them from spreading and further eroding people’s confidence in what is true. By working with a large Fake News dataset that has over 20000 articles, split roughly even between fake news articles and real news articles, that contains not only the content of the articles but the authors and title of the articles I am planning to apply this dataset to be able to create models that are able to distinguish between the two types of news.\n",
    "    \n",
    "Before I create the models themselves, I first start by creating a wordcloud of the top 500 words found in both the fake news articles and real news articles, just to get a high level understanding of what some of the key words might be when trying to detect real vs fake news. The second task I take on is applying preprocessing techniques to clean the text, removing stop words, and applying techniques such as stemming and lemmanization, which will later prove useful in building the models themselves. Stemming and Lemmanization are two techniques often used in sentiment analysis projects such as this one that help to transform words into their root words in the case of stemming, or to their correct canonical form in the case of lemmanization. By first converting words to their correct base, this will later ensure that the models are only dealing with the correct root word. \n",
    "\n",
    "\n",
    "Finally after getting every text article cleaned up, I then try out two different types of models, Logistic Regression and a LSTM Recurrent Neural Network model, to see which can ultimately do better on this specific task. By comparing a more common type of model in the case of the Logistic Regression Model with a more advanced model such as the LSTM model, I learn not only how a LSTM model works, but also get to compare the two models to see which does best on this particular task. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install nltk\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "#pip install tensorflow\n",
    "#pip install tensorflow_datasets\n",
    "#pip install wordcloud\n",
    "#pip install tensorflow_tex\n",
    "#pip install keras\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import LSTM, Bidirectional, Dense, Embedding, Dropout\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>Darrell Lucus</td>\n",
       "      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n",
       "      <td>Daniel J. Flynn</td>\n",
       "      <td>Ever get the feeling your life circles the rou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Why the Truth Might Get You Fired</td>\n",
       "      <td>Consortiumnews.com</td>\n",
       "      <td>Why the Truth Might Get You Fired October 29, ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>15 Civilians Killed In Single US Airstrike Hav...</td>\n",
       "      <td>Jessica Purkiss</td>\n",
       "      <td>Videos 15 Civilians Killed In Single US Airstr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Iranian woman jailed for fictional unpublished...</td>\n",
       "      <td>Howard Portnoy</td>\n",
       "      <td>Print \\nAn Iranian woman has been sentenced to...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20795</th>\n",
       "      <td>20795</td>\n",
       "      <td>Rapper T.I.: Trump a ’Poster Child For White S...</td>\n",
       "      <td>Jerome Hudson</td>\n",
       "      <td>Rapper T. I. unloaded on black celebrities who...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20796</th>\n",
       "      <td>20796</td>\n",
       "      <td>N.F.L. Playoffs: Schedule, Matchups and Odds -...</td>\n",
       "      <td>Benjamin Hoffman</td>\n",
       "      <td>When the Green Bay Packers lost to the Washing...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20797</th>\n",
       "      <td>20797</td>\n",
       "      <td>Macy’s Is Said to Receive Takeover Approach by...</td>\n",
       "      <td>Michael J. de la Merced and Rachel Abrams</td>\n",
       "      <td>The Macy’s of today grew from the union of sev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20798</th>\n",
       "      <td>20798</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>Alex Ansary</td>\n",
       "      <td>NATO, Russia To Hold Parallel Exercises In Bal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20799</th>\n",
       "      <td>20799</td>\n",
       "      <td>What Keeps the F-35 Alive</td>\n",
       "      <td>David Swanson</td>\n",
       "      <td>David Swanson is an author, activist, journa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20800 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                              title  \\\n",
       "0          0  House Dem Aide: We Didn’t Even See Comey’s Let...   \n",
       "1          1  FLYNN: Hillary Clinton, Big Woman on Campus - ...   \n",
       "2          2                  Why the Truth Might Get You Fired   \n",
       "3          3  15 Civilians Killed In Single US Airstrike Hav...   \n",
       "4          4  Iranian woman jailed for fictional unpublished...   \n",
       "...      ...                                                ...   \n",
       "20795  20795  Rapper T.I.: Trump a ’Poster Child For White S...   \n",
       "20796  20796  N.F.L. Playoffs: Schedule, Matchups and Odds -...   \n",
       "20797  20797  Macy’s Is Said to Receive Takeover Approach by...   \n",
       "20798  20798  NATO, Russia To Hold Parallel Exercises In Bal...   \n",
       "20799  20799                          What Keeps the F-35 Alive   \n",
       "\n",
       "                                          author  \\\n",
       "0                                  Darrell Lucus   \n",
       "1                                Daniel J. Flynn   \n",
       "2                             Consortiumnews.com   \n",
       "3                                Jessica Purkiss   \n",
       "4                                 Howard Portnoy   \n",
       "...                                          ...   \n",
       "20795                              Jerome Hudson   \n",
       "20796                           Benjamin Hoffman   \n",
       "20797  Michael J. de la Merced and Rachel Abrams   \n",
       "20798                                Alex Ansary   \n",
       "20799                              David Swanson   \n",
       "\n",
       "                                                    text  label  \n",
       "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1  \n",
       "1      Ever get the feeling your life circles the rou...      0  \n",
       "2      Why the Truth Might Get You Fired October 29, ...      1  \n",
       "3      Videos 15 Civilians Killed In Single US Airstr...      1  \n",
       "4      Print \\nAn Iranian woman has been sentenced to...      1  \n",
       "...                                                  ...    ...  \n",
       "20795  Rapper T. I. unloaded on black celebrities who...      0  \n",
       "20796  When the Green Bay Packers lost to the Washing...      0  \n",
       "20797  The Macy’s of today grew from the union of sev...      0  \n",
       "20798  NATO, Russia To Hold Parallel Exercises In Bal...      1  \n",
       "20799    David Swanson is an author, activist, journa...      1  \n",
       "\n",
       "[20800 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news=pd.read_csv(\"train.csv\")\n",
    "news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Data and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of each column of Dataframe :\n",
      "id         int64\n",
      "title     object\n",
      "author    object\n",
      "text      object\n",
      "label      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "dataTypeSeries = news.dtypes\n",
    "print('Data type of each column of Dataframe :')\n",
    "print(dataTypeSeries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id           0\n",
       "title      558\n",
       "author    1957\n",
       "text        39\n",
       "label        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id        0\n",
       "title     0\n",
       "author    0\n",
       "text      0\n",
       "label     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.fillna('Currently Missing',inplace=True)\n",
    "news.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These steps were to check to see what type of data was missing from the dataset, and instead of removing all the rows that had some value missing, I instead filled in all the missing values with a string labeled \"Currently Missing\". This was to avoid having to remove close to 2000 articles from the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10413\n",
       "0    10387\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_counts=news[\"label\"].value_counts()\n",
    "news_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that there is a very even split between real and fake news articles in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Currently Missing                                    1957\n",
       "Pam Key                                               243\n",
       "admin                                                 193\n",
       "Jerome Hudson                                         166\n",
       "Charlie Spiering                                      141\n",
       "                                                     ... \n",
       "Michael S. Schmidt, Mark Mazzetti and Matt Apuzzo       1\n",
       "Javier C. Hernández and Cao Li                          1\n",
       "Miss Marple                                             1\n",
       "Brendan Nyhan                                           1\n",
       "Wahid Azal                                              1\n",
       "Name: author, Length: 4202, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_counts=news[\"author\"].value_counts()\n",
    "author_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can get an idea of what type authors have multiple articles in the dataset. While the number of missing authors is the highest, we can see that Pam Key has 243 articles herself in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news=news[news[\"label\"]==1]\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "wordCloud = WordCloud(max_words = 500 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(fake_news[\"text\"]))\n",
    "plt.imshow(wordCloud, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at a wordcloud of the top 500 words from all the fake news articles, we can start to get a sense of what type of words tend to make up this article. Very quickly one will notice the prevalence of political words and names such as Trump, Hillary Clinton, Russia, election, and government. This may give us an idea that many fake news articles are very politically motivated and so this may mean that any political news article should be more closely screened as to whether or not its fake news."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_news=news[news[\"label\"]==0]\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "wordCloud = WordCloud(max_words = 500 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(real_news[\"text\"]))\n",
    "plt.imshow(wordCloud, interpolation = 'bilinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at this wordcloud of the top 500 words from real news articles, we can start to compare to the previous wordcloud. We can see that Trump is still one of the top words but should notice that Hilary Clinton is no longer one of the top words. We can also see how words such as country and president are now top words. This could tell us that articles about Hilary Clinton tend to be fake news whereas articles about Trump may tell a more accurate story. Overall though from looking at these two wordcloud together, one can see see how the prevalence of how this dataset is likely made up of predominantly political news articles, both real and fake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1=news.iloc[0, news.columns.get_loc('text')]\n",
    "text1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a check to see an an example of the actual content of one of the articles and to see what sort of cleaning of the article that will have to be done. We can see that there will be a little bit of cleaning required for the documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "\n",
    "def tokenizer_stemmer(text):\n",
    "    return[stemmer.stem(word) for word in text.split()]\n",
    "\n",
    "#tokenizer_stemmer(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "def lemmanizer(text):\n",
    "    return[lemmatizer.lemmatize(word) for word in text.split() if not word in stop_words]\n",
    "\n",
    "newtext=lemmanizer(text1)\n",
    "#newtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compare different types of lemmanizers vs snowball stemming algorithms on an article. By scanning through the two new sets of words, we can see that overall the lemmanizer appears to do a slightly better job of maintaining the correct word. Therefore I will use the lemmanizer algorithm and apply it to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def text_lemmanizer(article_text):\n",
    "\n",
    "    article_text = re.sub('[^a-zA-Z]', ' ', article_text)\n",
    "    article_text = article_text.lower().split()\n",
    "    article_txt = [lemmatizer.lemmatize(word) for word in article_text if word not in stop_words]\n",
    "    article_text = ' '.join(article_text)\n",
    "    article_text = ''.join(p for p in article_text if p not in punctuation)\n",
    "    \n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beyond just applying a lemmanizer to all the articles. I also wanted to clean the data further. In the function above I remove all numbers, convert all the words to lower case and then also remove all the common stop words from the dataset. This will be useful when I go to create the models so that only the most significant words get passed in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This will take a while to run\n",
    "news['lemmatized_text'] = news['text'].apply(text_lemmanizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        house dem aide we didn t even see comey s lett...\n",
       "1        ever get the feeling your life circles the rou...\n",
       "2        why the truth might get you fired october the ...\n",
       "3        videos civilians killed in single us airstrike...\n",
       "4        print an iranian woman has been sentenced to s...\n",
       "                               ...                        \n",
       "20795    rapper t i unloaded on black celebrities who m...\n",
       "20796    when the green bay packers lost to the washing...\n",
       "20797    the macy s of today grew from the union of sev...\n",
       "20798    nato russia to hold parallel exercises in balk...\n",
       "20799    david swanson is an author activist journalist...\n",
       "Name: lemmatized_text, Length: 20800, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news['lemmatized_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the data has been cleaned and preprocessed it is ready to be used to create a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=news['lemmatized_text'].values\n",
    "y=news['label'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf=TfidfVectorizer(strip_accents=None, lowercase=True, preprocessor=None)\n",
    "\n",
    "param_grid= [{'vect__ngram_range':[(1,1)],\n",
    "              'clf__penalty': ['l1','l2'],\n",
    "              'clf__C':[1.0,10.0,100.0]},\n",
    "             {'vect__ngram_range':[(1,1)],\n",
    "              'vect__use_idf':[False],\n",
    "              'vect__norm':[None],\n",
    "              'clf__penalty': ['l1','l2'],\n",
    "              'clf__C':[1.0,10.0,100.0]}\n",
    "            ]\n",
    "\n",
    "lr_tfidf=Pipeline([('vect',tfidf),\n",
    "                  ('clf', LogisticRegression(random_state=0,solver='liblinear'))])\n",
    "\n",
    "gs_lr_tfidf=GridSearchCV(lr_tfidf, param_grid, scoring='accuracy', cv=5,verbose=2, n_jobs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the logistic regression model I first use the TfidfVectorizer function, which allows me to first convert each word in a document to an individual token and then it calculates the tf-idf score for each document as well. The tf-idf score is essentially a ratio of how significant each word is in relation to other documents. I then apply the grid search algorithm to find out the best possible C value and which regularization penalty to use. This will all allow me to find the best possible logistic regression model for the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1) ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), total=   7.4s\n",
      "[CV] clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1) ...........\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.4s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), total=   7.7s\n",
      "[CV] clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1) ...........\n",
      "[CV]  clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), total=   7.4s\n",
      "[CV] clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1) ...........\n",
      "[CV]  clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), total=   7.3s\n",
      "[CV] clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1) ...........\n",
      "[CV]  clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), total=   7.2s\n",
      "[CV] clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1) ...........\n",
      "[CV]  clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), total=   7.4s\n",
      "[CV] clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1) ...........\n",
      "[CV]  clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), total=   7.3s\n",
      "[CV] clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1) ...........\n",
      "[CV]  clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), total=   7.4s\n",
      "[CV] clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1) ...........\n",
      "[CV]  clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), total=   7.3s\n",
      "[CV] clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1) ...........\n",
      "[CV]  clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), total=   7.3s\n",
      "[CV] clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1) ..........\n",
      "[CV]  clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), total=   7.8s\n",
      "[CV] clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1) ..........\n",
      "[CV]  clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), total=   7.7s\n",
      "[CV] clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1) ..........\n",
      "[CV]  clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), total=   7.6s\n",
      "[CV] clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1) ..........\n",
      "[CV]  clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), total=   7.7s\n",
      "[CV] clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1) ..........\n",
      "[CV]  clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), total=   7.7s\n",
      "[CV] clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1) ..........\n",
      "[CV]  clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), total=   7.8s\n",
      "[CV] clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1) ..........\n",
      "[CV]  clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), total=   8.0s\n",
      "[CV] clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1) ..........\n",
      "[CV]  clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), total=   7.9s\n",
      "[CV] clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1) ..........\n",
      "[CV]  clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), total=   7.8s\n",
      "[CV] clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1) ..........\n",
      "[CV]  clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), total=   7.9s\n",
      "[CV] clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), total=   7.9s\n",
      "[CV] clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), total=   7.8s\n",
      "[CV] clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), total=   7.8s\n",
      "[CV] clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), total=   7.9s\n",
      "[CV] clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), total=   7.8s\n",
      "[CV] clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), total=   8.4s\n",
      "[CV] clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), total=   8.3s\n",
      "[CV] clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), total=   8.5s\n",
      "[CV] clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), total=   8.4s\n",
      "[CV] clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1) .........\n",
      "[CV]  clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), total=   8.4s\n",
      "[CV] clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=   8.3s\n",
      "[CV] clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=   8.2s\n",
      "[CV] clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=   8.6s\n",
      "[CV] clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=   7.6s\n",
      "[CV] clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=   8.2s\n",
      "[CV] clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=  11.6s\n",
      "[CV] clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=  11.1s\n",
      "[CV] clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=  11.6s\n",
      "[CV] clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=  12.5s\n",
      "[CV] clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=1.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=  11.0s\n",
      "[CV] clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=   8.1s\n",
      "[CV] clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=   7.8s\n",
      "[CV] clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=   7.3s\n",
      "[CV] clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=   7.7s\n",
      "[CV] clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=   7.4s\n",
      "[CV] clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=  11.9s\n",
      "[CV] clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=  11.1s\n",
      "[CV] clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=  11.9s\n",
      "[CV] clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=  11.9s\n",
      "[CV] clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf__C=10.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=  13.6s\n",
      "[CV] clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=   7.5s\n",
      "[CV] clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=   7.6s\n",
      "[CV] clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=   7.3s\n",
      "[CV] clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=   7.7s\n",
      "[CV] clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l1, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=   7.8s\n",
      "[CV] clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=  14.2s\n",
      "[CV] clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=  13.6s\n",
      "[CV] clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=  13.2s\n",
      "[CV] clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=  12.8s\n",
      "[CV] clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False \n",
      "[CV]  clf__C=100.0, clf__penalty=l2, vect__ngram_range=(1, 1), vect__norm=None, vect__use_idf=False, total=  13.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  8.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('vect', TfidfVectorizer()),\n",
       "                                       ('clf',\n",
       "                                        LogisticRegression(random_state=0,\n",
       "                                                           solver='liblinear'))]),\n",
       "             n_jobs=1,\n",
       "             param_grid=[{'clf__C': [1.0, 10.0, 100.0],\n",
       "                          'clf__penalty': ['l1', 'l2'],\n",
       "                          'vect__ngram_range': [(1, 1)]},\n",
       "                         {'clf__C': [1.0, 10.0, 100.0],\n",
       "                          'clf__penalty': ['l1', 'l2'],\n",
       "                          'vect__ngram_range': [(1, 1)], 'vect__norm': [None],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##This takes roughly 10 minutes to run\n",
    "gs_lr_tfidf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 100.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1)} \n",
      "CV Accuracy: 0.959\n"
     ]
    }
   ],
   "source": [
    "print('Best parameter set: %s ' % gs_lr_tfidf.best_params_)\n",
    "print('CV Accuracy: %.3f' %gs_lr_tfidf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the previous code, I find that the best hyperparameters for this model involve using a C value of 100 and using the l2 regularization penalty. I also use a 1 word gram model in order to keep the training time down on the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.960\n"
     ]
    }
   ],
   "source": [
    "clf=gs_lr_tfidf.best_estimator_\n",
    "print('Test Accuracy: %.3f' %clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding the best hyperparameters for the model. I then wanted to apply and test the model on the testing set. I find that this logistic regression model is able to achieve an accruacy score of 96.0%, implying that this model is able to do an extremely accurate job of detecting whether or not an article is real or fake. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I had created a logistic regression model. I now wanted to create a LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model (Recurrent Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=news['lemmatized_text']\n",
    "target=news['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 128\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size)\n",
    "tokenizer.fit_on_texts(features)\n",
    "train_sequences = tokenizer.texts_to_sequences(features)\n",
    "padded_train = pad_sequences(train_sequences,maxlen = 50, padding = 'post', truncating = 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I am converting all the words in the \"lemmatized_text\" column of the dataset into individual tokens for each of the unique words in the articles. In this process I select only the top 10000 words from all the articles and convert it into a sequence of integers. I then use the pad_sequence function to connvert the new sequence into a 2D numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_final=np.array(padded_train)\n",
    "y_final=np.array(target)\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(x_final, y_final, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 128)         1280000   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 1,723,777\n",
      "Trainable params: 1,723,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is the best model I was able to create using a LSTM model. By first using an embedding layer, which is a feature-learning technique, this allows the model to learn the salient and important words and features from the articles. I then apply a LSTM layer, which although slightly complicated to fully explain, has been shown to work extremely well for modeling long sequences such as text articles. This layer helps to remember the important features and words from the previous inputs, which is what allows it to be successful with complicated sequence tasks such as this one. Then I apply three dropout and dense layers, to try to prevent overfitting and as well make sure the entire model is connected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "160/160 [==============================] - 22s 136ms/step - loss: 0.3627 - accuracy: 0.8337 - val_loss: 0.2318 - val_accuracy: 0.9148\n",
      "Epoch 2/5\n",
      "160/160 [==============================] - 22s 140ms/step - loss: 0.1459 - accuracy: 0.9561 - val_loss: 0.2833 - val_accuracy: 0.9171\n",
      "Epoch 3/5\n",
      "160/160 [==============================] - 22s 134ms/step - loss: 0.0782 - accuracy: 0.9765 - val_loss: 0.3682 - val_accuracy: 0.9071\n",
      "Epoch 4/5\n",
      "160/160 [==============================] - 21s 133ms/step - loss: 0.0509 - accuracy: 0.9881 - val_loss: 0.3481 - val_accuracy: 0.9132\n",
      "Epoch 5/5\n",
      "160/160 [==============================] - 22s 135ms/step - loss: 0.0363 - accuracy: 0.9893 - val_loss: 0.3717 - val_accuracy: 0.9125\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size = 64, validation_split = 0.3, epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running this model, we can see how the model's accuracy drastically increases after the first epoch but then stays around the same through the final 4 epochs. We can also see from this output that the model still may be slightly overfitting the data, as the accuracy on the training set is always slightly higher than the accuracy on the validation set. Overall though we can be happy with seeing this model being able to achieve roughly 91% accuracy on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictions=model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9056089743589744\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test,model_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this result we can see that this particular LSTM model is able to achieve 90.56% accuracy on the testing set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusions and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By first cleaning up the text data, removing stop words, and then converting all the words in the articles to their root lemma, this allow both models to be very successful in being able to predict whether an article was real or fake. \n",
    "When comparing the Logistic Regression Model and the LSTM model, the Logistic Regression Model does a better job of making accurate predictions on this data set. Overall one can see that the logistic regression model was able to achieve on accuracy of 96.0% on the testing set, whereas the LSTM model was only able to achieve an accuracy of 90.4% on the testing set. Overall both these models do an extremely good job of being able to predict whether or not an article should be classified as real or fake news. \n",
    "\n",
    "While being able to achieve scores as high on 96% and 90.4% is very exciting on its own, this project could be built on heavily on the future. Firstly, while I tried to create many different models using the LSTM and the embedding layers, there is other types of LSTM models that could be explored in the future. By potentially trying out differing types of LSTM variations or even trying out a more recent approach such as a Gated Recurrent Unit (GRU) model, I could hope to improve the accuracy of this particular type of model even further. I could also have tried with the Logistic Regression model different sizes of bag of word models, beyond just the one word sized model. \n",
    "\n",
    "Beyond just ways to increase the accuracy of the models themselves, the work done in this project could be used heavily to start identifying news articles outside of the one in this dataset. Being able to detect and remove fake news articles early on from sites such as Twitter or Facebook has huge societal implications, and maintaining faith in science and the truth in general, and using these types of models could be one way to begin trying to solve this issue. By utilizing these types of models to detect fake news articles early on could help prevent the spread of fake news and help people begin relying on more trusted sources in the future. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
     "Works Cited\n",
    "\n",
    "“Fake News.” Kaggle, www.kaggle.com/c/fake-news. \n",
    "\n",
    "Phi, Michael. “Illustrated Guide to LSTM's and GRU's: A Step by Step Explanation.” Medium, Towards Data Science, 28 June 2020, towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21. \n",
    "\n",
    "Raschka, Sebastian, and Vahid Mirjalili. Python Machine Learning. Packt Publishing Limited, 2015. \n",
    "\n",
    "Sinha, Nimesh. “Understanding LSTM and Its Quick Implementation in Keras for Sentiment Analysis.” Medium, Towards Data Science, 3 Mar. 2018, towardsdatascience.com/understanding-lstm-and-its-quick-implementation-in-keras-for-sentiment-analysis-af410fd85b47. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
